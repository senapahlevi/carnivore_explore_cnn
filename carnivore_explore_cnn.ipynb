{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "carnivore_explore_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMf4o6y6LyhyvqH5gsO0xcy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdcUkqEJPkfa"
      },
      "source": [
        "Initial Setting for save connect with google drive because colab not support local so for efficient time just upload to google colab or download using kaggle command API "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYh7LMZ_O61B"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiiQncIph6eY"
      },
      "source": [
        "Using latest tensorflow 2.4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7-Fxiv2hzpd",
        "outputId": "9c7656ee-e72c-4e6c-f1d6-157a7793838d"
      },
      "source": [
        "import tensorflow\r\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIE-jjr9h6Vu"
      },
      "source": [
        "Import Library for Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYh-lTKlPJVc"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import sklearn\r\n",
        "import mathimport warnings\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQzqKRuGPcuc"
      },
      "source": [
        "PRETRAINED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcs2fcyuPSAT"
      },
      "source": [
        "import keras\r\n",
        "from keras.applications.inception_v3 import InceptionV3\r\n",
        "from keras.models import Model,load_model\r\n",
        "\r\n",
        "\r\n",
        "conv_base = InceptionV3(weights = 'imagenet',include_top=False,input_shape=(300,300,3))\r\n",
        "\r\n",
        "output = conv_base.layers[-1].output\r\n",
        "output = keras.layers.Flatten()(output)\r\n",
        "model_tl = Model(conv_base.input, output)\r\n",
        "\r\n",
        "model_tl.trainable = False\r\n",
        "for layer in model_tl.layers:\r\n",
        "  layer.trainable = False\r\n",
        "\r\n",
        "layers = [(layer,layer.name,layer.trainable)for layer in\r\n",
        "          model_tl.layers]\r\n",
        "model_layers = pd.DataFrame(layers,columns=['layer Type','Layer Name','Layer Trainable'])\r\n",
        "\r\n",
        "print(model_layers)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv00b93uiRy7"
      },
      "source": [
        "Pre processing dataset model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oit_RHZhPSDz"
      },
      "source": [
        "#preproses dan data augmentasi\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense,Dropout,Conv2D,MaxPool2D,Flatten\r\n",
        "\r\n",
        "test_size = 400\r\n",
        "batch_size =32\r\n",
        "epochs=25\r\n",
        "\r\n",
        "#dataset \r\n",
        "train_path = '/content/drive/MyDrive/TALatihan/carnivores/train'\r\n",
        "test_path = '/content/drive/MyDrive/TALatihan/carnivores/test'\r\n",
        "\r\n",
        "target_size = (300,300) #resize all images to 300x300\r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3,  \r\n",
        "                                   rotation_range=50,\r\n",
        "                                   width_shift_range=0.2, \r\n",
        "                                   height_shift_range=0.2, \r\n",
        "                                   shear_range=0.2,\r\n",
        "                                   horizontal_flip=True,\r\n",
        "                                   brightness_range = [0.8, 1.2],\r\n",
        "                                   fill_mode='nearest',        \r\n",
        "                                   validation_split=0.2)\r\n",
        "\r\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\r\n",
        "\r\n",
        "#the list of classes will be automatically inferred from the subdirectory names/stucture under train_dir\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "                  train_path,\r\n",
        "                  target_size=target_size,#  \r\n",
        "                  batch_size=batch_size,\r\n",
        "                  class_mode='categorical',\r\n",
        "                  subset='training')\r\n",
        "validation_generator = train_datagen.flow_from_directory(\r\n",
        "                       train_path,\r\n",
        "                       target_size=target_size,\r\n",
        "                       batch_size=batch_size,\r\n",
        "                       class_mode='categorical',\r\n",
        "                       subset='validation')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_ebAo7hirlg"
      },
      "source": [
        "BUILD MODEL ARCHITECTURE and connect with resume training per epoch because colab free version has limited there are: max 12 training with gpu automatically restart or no activitiy more than 1 hours will reconnect again so using save model weight check point for save every epoch training with format ((.hdf5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfm_ce8YikfK"
      },
      "source": [
        "#building model arsitektur\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense,Dropout,Conv2D,MaxPool2D,Flatten\r\n",
        "from keras import optimizers\r\n",
        "#building a linear stack of layers with the sequential model\r\n",
        "model = Sequential()\r\n",
        "model.add(model_tl)\r\n",
        "\r\n",
        "#hidden layer\r\n",
        "model.add(Dense(128,activation='relu'))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "#output layer\r\n",
        "model.add(Dense(4,activation='softmax'))\r\n",
        "#resume training abis internet dc/mati lampu\r\n",
        "model.load_weights('/content/drive/MyDrive/TALatihan/CP/epochs:025-val_acc:0.993.hdf5') \r\n",
        "#compiling the sequential model\r\n",
        "model.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\r\n",
        "              metrics=['acc'])\r\n",
        "print(model.summary())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vga4RVhYPSHv"
      },
      "source": [
        "#Saving model checkpoint\r\n",
        "from keras.callbacks import *\r\n",
        "#/content/drive/MyDrive/TALatihan/CP\r\n",
        "\r\n",
        "filepath ='/content/drive/MyDrive/TALatihan/CP/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5'\r\n",
        "checkpoint = ModelCheckpoint(filepath,\r\n",
        "                             monitor='val_acc',\r\n",
        "                             verbose=1,\r\n",
        "                             save_best_only=False,\r\n",
        "                             save_freq='epoch',\r\n",
        "                             mode='max')\r\n",
        "callbacks_list=[checkpoint]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWWAWywAPSL0"
      },
      "source": [
        "\r\n",
        "#model performance\r\n",
        "#model evaluation\r\n",
        "scores_train = model.evaluate(train_generator,verbose=1)\r\n",
        "scores_validation = model.evaluate(validation_generator,verbose=1)\r\n",
        "print(\"Train Accuracy: %.2f%%\" % (scores_train[1]*100))\r\n",
        "print(\"validation akurasi : %.2f%% \"%(scores_validation[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYHT7c-DPSPU"
      },
      "source": [
        "def LearningCurve(history):\r\n",
        "#summarize history for akurasi\r\n",
        "  plt.plot(history.history['acc'])\r\n",
        "  plt.plot(history.history['val_acc'])\r\n",
        "  plt.title('model accurasi')\r\n",
        "  plt.ylabel('accurasi')\r\n",
        "  plt.xlabel('epoch')\r\n",
        "  plt.legend(['train','validation'],loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGqlPdhiPSfp"
      },
      "source": [
        "\r\n",
        "def LearningCurve(history):\r\n",
        "#summarize history for akurasi\r\n",
        "  plt.plot(history.history['acc'])\r\n",
        "  plt.plot(history.history['val_acc'])\r\n",
        "  plt.title('model accurasi')\r\n",
        "  plt.ylabel('accurasi')\r\n",
        "  plt.xlabel('epoch')\r\n",
        "  plt.legend(['train','validation'],loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjkdXhygf6I4"
      },
      "source": [
        "#we take the ceiling because we dont drop the remainder of the batch\r\n",
        "import math\r\n",
        "compute_steps_per_epoch = lambda x: int(math.ceil(1. * x / batch_size))\r\n",
        "test_steps = compute_steps_per_epoch(test_size)\r\n",
        "test_generator = test_datagen.flow_from_directory(\r\n",
        "                 test_path,\r\n",
        "                 target_size=target_size, \r\n",
        "                 batch_size=batch_size,\r\n",
        "                 class_mode=None,\r\n",
        "                 shuffle=False)\r\n",
        "test_generator.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doTEBR0xf6Z-"
      },
      "source": [
        "#Calling the saved model for making predictions\r\n",
        "tl_img_aug_cnn = load_model(model_weight_file)\r\n",
        "pred=tl_img_aug_cnn.predict(test_generator,\r\n",
        "                            verbose=1,\r\n",
        "                            steps=test_steps)\r\n",
        "predicted_class_indices=np.argmax(pred,axis=1)\r\n",
        "labels = (test_generator.class_indices)\r\n",
        "labels = dict((v,k) for k,v in labels.items())\r\n",
        "predictions = [labels[k] for k in predicted_class_indices]\r\n",
        "filenames=test_generator.filenames\r\n",
        "results=pd.DataFrame({\"Filename\":filenames,\r\n",
        "                      \"Predictions\":predictions})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXH5gJdHhizo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
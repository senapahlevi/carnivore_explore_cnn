{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "carnivore_explore_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjW7qfzHrqFX1a3yVZcskp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdcUkqEJPkfa"
      },
      "source": [
        "Initial Setting for save connect with google drive because colab not support local so for efficient time just upload to google colab or download using kaggle command API "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYh7LMZ_O61B"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiiQncIph6eY"
      },
      "source": [
        "Using latest tensorflow 2.4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7-Fxiv2hzpd",
        "outputId": "9c7656ee-e72c-4e6c-f1d6-157a7793838d"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIE-jjr9h6Vu"
      },
      "source": [
        "Import Library for Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYh-lTKlPJVc"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import mathimport warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQzqKRuGPcuc"
      },
      "source": [
        "PRETRAINED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcs2fcyuPSAT"
      },
      "source": [
        "import keras\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.models import Model,load_model\n",
        "\n",
        "\n",
        "conv_base = InceptionV3(weights = 'imagenet',include_top=False,input_shape=(300,300,3))\n",
        "\n",
        "output = conv_base.layers[-1].output\n",
        "output = keras.layers.Flatten()(output)\n",
        "model_tl = Model(conv_base.input, output)\n",
        "\n",
        "model_tl.trainable = False\n",
        "for layer in model_tl.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "layers = [(layer,layer.name,layer.trainable)for layer in\n",
        "          model_tl.layers]\n",
        "model_layers = pd.DataFrame(layers,columns=['layer Type','Layer Name','Layer Trainable'])\n",
        "\n",
        "print(model_layers)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv00b93uiRy7"
      },
      "source": [
        "Pre processing dataset model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oit_RHZhPSDz"
      },
      "source": [
        "#preproses dan data augmentasi\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Conv2D,MaxPool2D,Flatten\n",
        "\n",
        "test_size = 400\n",
        "batch_size =32\n",
        "epochs=25\n",
        "\n",
        "#dataset \n",
        "train_path = '/content/drive/MyDrive/TALatihan/carnivores/train'\n",
        "test_path = '/content/drive/MyDrive/TALatihan/carnivores/test'\n",
        "\n",
        "target_size = (300,300) #resize all images to 300x300\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3,  \n",
        "                                   rotation_range=50,\n",
        "                                   width_shift_range=0.2, \n",
        "                                   height_shift_range=0.2, \n",
        "                                   shear_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   brightness_range = [0.8, 1.2],\n",
        "                                   fill_mode='nearest',        \n",
        "                                   validation_split=0.2)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#the list of classes will be automatically inferred from the subdirectory names/stucture under train_dir\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                  train_path,\n",
        "                  target_size=target_size,#  \n",
        "                  batch_size=batch_size,\n",
        "                  class_mode='categorical',\n",
        "                  subset='training')\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "                       train_path,\n",
        "                       target_size=target_size,\n",
        "                       batch_size=batch_size,\n",
        "                       class_mode='categorical',\n",
        "                       subset='validation')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_ebAo7hirlg"
      },
      "source": [
        "BUILD MODEL ARCHITECTURE and connect with resume training per epoch because colab free version has limited there are: max 12 training with gpu automatically restart or no activitiy more than 1 hours will reconnect again so using save model weight check point for save every epoch training with format ((.hdf5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfm_ce8YikfK"
      },
      "source": [
        "#building model arsitektur\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Conv2D,MaxPool2D,Flatten\n",
        "from keras import optimizers\n",
        "#building a linear stack of layers with the sequential model\n",
        "model = Sequential()\n",
        "model.add(model_tl)\n",
        "\n",
        "#hidden layer\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "#output layer\n",
        "model.add(Dense(4,activation='softmax'))\n",
        "#resume training abis internet dc/mati lampu\n",
        "model.load_weights('/content/drive/MyDrive/TALatihan/CP/epochs:025-val_acc:0.993.hdf5') \n",
        "#compiling the sequential model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYL0qd0O68Ae"
      },
      "source": [
        "Saving Model Checkpoint every epochs because google colab has limit like game use check point and dont waste time and be efficient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vga4RVhYPSHv"
      },
      "source": [
        "#Saving model checkpoint\n",
        "from keras.callbacks import *\n",
        "#/content/drive/MyDrive/TALatihan/CP\n",
        "\n",
        "filepath ='/content/drive/MyDrive/TALatihan/CP/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=False,\n",
        "                             save_freq='epoch',\n",
        "                             mode='max')\n",
        "callbacks_list=[checkpoint]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWWAWywAPSL0"
      },
      "source": [
        "\n",
        "#model performance\n",
        "#model evaluation\n",
        "scores_train = model.evaluate(train_generator,verbose=1)\n",
        "scores_validation = model.evaluate(validation_generator,verbose=1)\n",
        "print(\"Train Accuracy: %.2f%%\" % (scores_train[1]*100))\n",
        "print(\"validation akurasi : %.2f%% \"%(scores_validation[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYHT7c-DPSPU"
      },
      "source": [
        "def LearningCurve(history):\n",
        "#summarize history for akurasi\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accurasi')\n",
        "  plt.ylabel('accurasi')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train','validation'],loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGqlPdhiPSfp"
      },
      "source": [
        "\n",
        "def LearningCurve(history):\n",
        "#summarize history for akurasi\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accurasi')\n",
        "  plt.ylabel('accurasi')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train','validation'],loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjkdXhygf6I4"
      },
      "source": [
        "#we take the ceiling because we dont drop the remainder of the batch\n",
        "import math\n",
        "compute_steps_per_epoch = lambda x: int(math.ceil(1. * x / batch_size))\n",
        "test_steps = compute_steps_per_epoch(test_size)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "                 test_path,\n",
        "                 target_size=target_size, \n",
        "                 batch_size=batch_size,\n",
        "                 class_mode=None,\n",
        "                 shuffle=False)\n",
        "test_generator.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkqEgk-S66Z1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doTEBR0xf6Z-"
      },
      "source": [
        "#Calling the saved model for making predictions\n",
        "tl_img_aug_cnn = load_model(model_weight_file)\n",
        "pred=tl_img_aug_cnn.predict(test_generator,\n",
        "                            verbose=1,\n",
        "                            steps=test_steps)\n",
        "predicted_class_indices=np.argmax(pred,axis=1)\n",
        "labels = (test_generator.class_indices)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "predictions = [labels[k] for k in predicted_class_indices]\n",
        "filenames=test_generator.filenames\n",
        "results=pd.DataFrame({\"Filename\":filenames,\n",
        "                      \"Predictions\":predictions})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVt0_ibN6adE"
      },
      "source": [
        "create a function for visualizing model performance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXH5gJdHhizo"
      },
      "source": [
        "import seaborn as sns\n",
        "def PerformanceReports(conf_matrix,class_report,labels):\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(conf_matrix, annot=True,ax=ax)\n",
        "    #labels, title and ticks\n",
        "    ax.set_xlabel('Predicted labels')\n",
        "    ax.set_ylabel('True labels')\n",
        "    ax.set_title('Confusion Matrix')\n",
        "    ax.xaxis.set_ticklabels(labels)\n",
        "    ax.yaxis.set_ticklabels(labels)\n",
        "    plt.show()\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T,  \n",
        "                annot=True,ax=ax)\n",
        "    ax.set_title('Classification Report')\n",
        "    plt.show()\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "labels=['cheetah','hyena','jaguar','tiger']\n",
        "test_labels = [fn.split('/')[0] for fn in filenames]\n",
        "cm=confusion_matrix(test_labels,predictions)\n",
        "print(cm)\n",
        "cr=classification_report(test_labels, predictions)\n",
        "class_report=classification_report(test_labels, predictions,\n",
        "                                   target_names=labels,\n",
        "                                   output_dict=True)\n",
        "print(cr)\n",
        "PerformanceReports(cm,class_report,labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}